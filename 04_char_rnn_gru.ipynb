{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3091631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "class GRUModel(torch.nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size, dict_size):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dict_size = dict_size\n",
    "        self.embeddings = torch.nn.Embedding(dict_size, hidden_size)\n",
    "        self.gru = torch.nn.GRU(\n",
    "            input_size = hidden_size,\n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True,\n",
    "        )\n",
    "        self.projection = torch.nn.Linear(hidden_size, dict_size)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        x = self.embeddings(x)\n",
    "        y, h = self.gru(x, h)\n",
    "        return self.projection(y), h\n",
    "\n",
    "    def zero_state(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size) if batch_size is not None else torch.zeros(self.num_layers, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2835e316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5458199\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "all_shakespeare = requests.get(\"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\").content.decode()\n",
    "print(len(all_shakespeare))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7dac074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f', ')', '|', '6', 'q', \"'\", 'U', 'Q', 'u', 'T', 'p', ',', 'S', 'y', ' ', 'N', '(', 'P', 'X', 'r', 'n', 'M', ':', 'v', '\"', '[', 'A', 'd', '2', 'F', 'W', '>', 'K', 'h', 'E', 'c', 'e', 'k', 'O', '~', '!', '8', 'C', 'm', 'B', '7', '`', 'L', 'j', '=', '.', 'a', 's', '_', '3', 'Y', 'i', 'x', 'I', '*', 'Z', 'w', '@', '9', 't', 'l', '}', '0', '5', ']', '<', '?', 'G', 'z', 'R', 'J', 'o', '\\n', '1', '4', ';', '#', '%', '&', 'H', 'V', 'D', '/', '-', 'b', 'g', '<start>', '<end>', '<empty>']\n",
      "94\n",
      "{'f': 0, ')': 1, '|': 2, '6': 3, 'q': 4, \"'\": 5, 'U': 6, 'Q': 7, 'u': 8, 'T': 9, 'p': 10, ',': 11, 'S': 12, 'y': 13, ' ': 14, 'N': 15, '(': 16, 'P': 17, 'X': 18, 'r': 19, 'n': 20, 'M': 21, ':': 22, 'v': 23, '\"': 24, '[': 25, 'A': 26, 'd': 27, '2': 28, 'F': 29, 'W': 30, '>': 31, 'K': 32, 'h': 33, 'E': 34, 'c': 35, 'e': 36, 'k': 37, 'O': 38, '~': 39, '!': 40, '8': 41, 'C': 42, 'm': 43, 'B': 44, '7': 45, '`': 46, 'L': 47, 'j': 48, '=': 49, '.': 50, 'a': 51, 's': 52, '_': 53, '3': 54, 'Y': 55, 'i': 56, 'x': 57, 'I': 58, '*': 59, 'Z': 60, 'w': 61, '@': 62, '9': 63, 't': 64, 'l': 65, '}': 66, '0': 67, '5': 68, ']': 69, '<': 70, '?': 71, 'G': 72, 'z': 73, 'R': 74, 'J': 75, 'o': 76, '\\n': 77, '1': 78, '4': 79, ';': 80, '#': 81, '%': 82, '&': 83, 'H': 84, 'V': 85, 'D': 86, '/': 87, '-': 88, 'b': 89, 'g': 90, '<start>': 91, '<end>': 92, '<empty>': 93}\n"
     ]
    }
   ],
   "source": [
    "dictionary = list(set(all_shakespeare))\n",
    "dictionary.append(\"<start>\")\n",
    "dictionary.append(\"<end>\")\n",
    "dictionary.append(\"<empty>\")\n",
    "print(dictionary)\n",
    "print(len(dictionary))\n",
    "\n",
    "sym2idx = {s: i for i, s in enumerate(dictionary)}\n",
    "print(sym2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4513ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.2531, -0.2977, -0.7140,  ..., -0.3242,  0.2216,  0.6172],\n",
      "        [-0.0093,  1.2123,  0.7450,  ...,  1.6020,  0.4401,  0.2959],\n",
      "        [-0.8178,  0.3585, -0.9875,  ..., -0.5761,  0.0409, -0.4539],\n",
      "        ...,\n",
      "        [-0.6046,  1.1757,  0.9760,  ..., -0.1582, -0.5503,  0.8111],\n",
      "        [-0.3206,  0.3883, -0.5169,  ..., -0.1127,  0.5922,  0.7392],\n",
      "        [ 1.4290,  1.1875,  0.2454,  ...,  0.6470,  1.1186,  0.5240]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0857, -0.0580, -0.0820,  ...,  0.0054, -0.0045,  0.0139],\n",
      "        [ 0.0072, -0.0568,  0.0318,  ..., -0.0798,  0.0056, -0.0280],\n",
      "        [-0.0411, -0.0668,  0.0286,  ..., -0.0289,  0.0539, -0.0095],\n",
      "        ...,\n",
      "        [ 0.0362,  0.0871, -0.0809,  ...,  0.0282,  0.0326, -0.0023],\n",
      "        [ 0.0553, -0.0223, -0.0495,  ..., -0.0634,  0.0587,  0.0255],\n",
      "        [ 0.0491, -0.0529,  0.0266,  ..., -0.0712, -0.0881,  0.0243]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0854,  0.0434,  0.0445,  ..., -0.0745, -0.0291, -0.0358],\n",
      "        [ 0.0560,  0.0599,  0.0668,  ..., -0.0511,  0.0055,  0.0272],\n",
      "        [ 0.0077, -0.0756,  0.0554,  ...,  0.0210, -0.0865, -0.0607],\n",
      "        ...,\n",
      "        [-0.0151, -0.0869, -0.0696,  ..., -0.0401,  0.0597, -0.0734],\n",
      "        [ 0.0823, -0.0331, -0.0189,  ...,  0.0417,  0.0750,  0.0671],\n",
      "        [-0.0388,  0.0119,  0.0087,  ..., -0.0131, -0.0055, -0.0585]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0471,  0.0309, -0.0234,  0.0589, -0.0115,  0.0402, -0.0834, -0.0784,\n",
      "        -0.0616,  0.0313,  0.0417,  0.0226,  0.0016,  0.0029, -0.0662,  0.0586,\n",
      "         0.0010,  0.0393, -0.0647,  0.0864, -0.0003,  0.0147,  0.0682, -0.0137,\n",
      "         0.0183, -0.0277, -0.0395,  0.0301,  0.0644,  0.0337,  0.0124,  0.0713,\n",
      "         0.0281, -0.0259,  0.0578,  0.0224,  0.0272, -0.0689,  0.0106,  0.0650,\n",
      "         0.0829, -0.0103, -0.0143,  0.0829, -0.0055, -0.0374,  0.0072, -0.0646,\n",
      "         0.0187,  0.0696, -0.0274, -0.0702, -0.0792, -0.0313, -0.0468, -0.0642,\n",
      "        -0.0150, -0.0226, -0.0330, -0.0808,  0.0036,  0.0675,  0.0281, -0.0175,\n",
      "         0.0830, -0.0181,  0.0530, -0.0441,  0.0391, -0.0259, -0.0601,  0.0002,\n",
      "         0.0869, -0.0466,  0.0349, -0.0573, -0.0355,  0.0710,  0.0388,  0.0758,\n",
      "         0.0016, -0.0099, -0.0535, -0.0625,  0.0494,  0.0360,  0.0391,  0.0865,\n",
      "        -0.0069,  0.0688,  0.0569,  0.0420, -0.0330,  0.0444, -0.0585, -0.0427,\n",
      "         0.0499, -0.0271,  0.0777, -0.0843,  0.0670,  0.0484, -0.0142, -0.0237,\n",
      "        -0.0322, -0.0824, -0.0010,  0.0674, -0.0811, -0.0089, -0.0278,  0.0731,\n",
      "         0.0646, -0.0405,  0.0400, -0.0418, -0.0853, -0.0081,  0.0649, -0.0299,\n",
      "        -0.0128,  0.0028, -0.0474, -0.0535, -0.0614, -0.0723, -0.0319, -0.0093,\n",
      "         0.0684,  0.0211,  0.0812, -0.0696, -0.0431, -0.0054,  0.0258, -0.0360,\n",
      "         0.0556, -0.0372,  0.0700,  0.0316, -0.0491, -0.0410,  0.0524,  0.0547,\n",
      "         0.0425, -0.0497,  0.0365, -0.0743, -0.0190,  0.0709,  0.0466,  0.0597,\n",
      "        -0.0839, -0.0282,  0.0515, -0.0515, -0.0012,  0.0029,  0.0457, -0.0515,\n",
      "         0.0704,  0.0441,  0.0538, -0.0330,  0.0249, -0.0843, -0.0020,  0.0764,\n",
      "         0.0031,  0.0270,  0.0358, -0.0246,  0.0179,  0.0710, -0.0217, -0.0532,\n",
      "        -0.0334,  0.0586,  0.0391, -0.0853,  0.0515, -0.0878,  0.0585,  0.0146,\n",
      "        -0.0551,  0.0871, -0.0839,  0.0269,  0.0446,  0.0695,  0.0484, -0.0605,\n",
      "         0.0171,  0.0024, -0.0296,  0.0103,  0.0049,  0.0829, -0.0269,  0.0400,\n",
      "        -0.0248,  0.0236,  0.0147, -0.0082, -0.0775, -0.0123,  0.0735,  0.0754,\n",
      "         0.0883, -0.0596,  0.0191,  0.0321,  0.0691, -0.0669,  0.0473, -0.0066,\n",
      "        -0.0810,  0.0569,  0.0064,  0.0780,  0.0814,  0.0620,  0.0755,  0.0688,\n",
      "        -0.0189,  0.0209,  0.0876,  0.0376,  0.0491,  0.0253,  0.0076, -0.0577,\n",
      "         0.0589, -0.0817, -0.0119, -0.0688, -0.0342, -0.0402, -0.0606, -0.0428,\n",
      "         0.0046, -0.0426, -0.0771,  0.0703, -0.0512,  0.0691, -0.0645,  0.0061,\n",
      "         0.0251,  0.0268,  0.0487,  0.0161, -0.0568,  0.0231,  0.0580,  0.0644,\n",
      "        -0.0511,  0.0793,  0.0333,  0.0618,  0.0443,  0.0756,  0.0804,  0.0603,\n",
      "         0.0825,  0.0768, -0.0828,  0.0812,  0.0853,  0.0528,  0.0418,  0.0243,\n",
      "        -0.0134, -0.0597, -0.0217, -0.0061,  0.0718, -0.0188,  0.0622, -0.0594,\n",
      "        -0.0061, -0.0833,  0.0474, -0.0616, -0.0312,  0.0604,  0.0447, -0.0708,\n",
      "         0.0514, -0.0261,  0.0838,  0.0053, -0.0309,  0.0719,  0.0428, -0.0353,\n",
      "         0.0241, -0.0012,  0.0013, -0.0739,  0.0351,  0.0779, -0.0311,  0.0744,\n",
      "        -0.0830, -0.0466,  0.0119,  0.0337,  0.0883, -0.0704,  0.0618, -0.0512,\n",
      "        -0.0106,  0.0018, -0.0329,  0.0454, -0.0385, -0.0843, -0.0676,  0.0637,\n",
      "         0.0848,  0.0197, -0.0071,  0.0691, -0.0784, -0.0777, -0.0018, -0.0821,\n",
      "        -0.0234, -0.0081, -0.0819, -0.0021,  0.0033, -0.0056, -0.0204, -0.0374,\n",
      "        -0.0526, -0.0467, -0.0852, -0.0083, -0.0866, -0.0811,  0.0401, -0.0329,\n",
      "        -0.0008,  0.0872,  0.0392, -0.0222, -0.0497,  0.0162,  0.0245, -0.0736,\n",
      "        -0.0154, -0.0569,  0.0556,  0.0511,  0.0142, -0.0681,  0.0584, -0.0556,\n",
      "         0.0585, -0.0131, -0.0093,  0.0020,  0.0013, -0.0437, -0.0691, -0.0417,\n",
      "        -0.0484, -0.0818, -0.0205,  0.0252, -0.0269, -0.0160, -0.0158, -0.0305,\n",
      "         0.0474,  0.0040, -0.0883, -0.0094, -0.0142,  0.0602,  0.0072,  0.0265],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0508, -0.0063, -0.0747, -0.0430,  0.0238, -0.0736,  0.0030, -0.0685,\n",
      "        -0.0675, -0.0118, -0.0770,  0.0246,  0.0587, -0.0883, -0.0174, -0.0066,\n",
      "        -0.0378,  0.0671, -0.0151,  0.0531,  0.0822,  0.0105, -0.0478,  0.0855,\n",
      "        -0.0520,  0.0431, -0.0245,  0.0430,  0.0214, -0.0375, -0.0703,  0.0238,\n",
      "         0.0069,  0.0650, -0.0840,  0.0472,  0.0076, -0.0387, -0.0598, -0.0687,\n",
      "        -0.0597,  0.0624, -0.0435,  0.0351,  0.0399,  0.0478,  0.0762, -0.0243,\n",
      "         0.0086, -0.0260,  0.0060, -0.0015,  0.0801, -0.0195, -0.0499,  0.0317,\n",
      "        -0.0413, -0.0294,  0.0121,  0.0197,  0.0267, -0.0360,  0.0801,  0.0725,\n",
      "         0.0204,  0.0049,  0.0621, -0.0132,  0.0807,  0.0466, -0.0883,  0.0212,\n",
      "        -0.0717,  0.0544,  0.0285, -0.0007,  0.0415, -0.0540,  0.0782, -0.0025,\n",
      "        -0.0157, -0.0294,  0.0802,  0.0800, -0.0485, -0.0156,  0.0458, -0.0511,\n",
      "         0.0183,  0.0597, -0.0045, -0.0692,  0.0448, -0.0338, -0.0012,  0.0192,\n",
      "         0.0712, -0.0447,  0.0716,  0.0552,  0.0643,  0.0099,  0.0727,  0.0683,\n",
      "         0.0597, -0.0342,  0.0425,  0.0368, -0.0245,  0.0442, -0.0652,  0.0841,\n",
      "         0.0078,  0.0119, -0.0666, -0.0360,  0.0357,  0.0153, -0.0161, -0.0101,\n",
      "        -0.0070, -0.0250,  0.0319, -0.0241, -0.0231, -0.0025, -0.0381,  0.0510,\n",
      "         0.0475, -0.0038, -0.0542, -0.0681,  0.0841, -0.0065, -0.0763, -0.0682,\n",
      "        -0.0856, -0.0122,  0.0759,  0.0070,  0.0645,  0.0590, -0.0137,  0.0664,\n",
      "        -0.0285,  0.0633, -0.0601, -0.0777,  0.0662, -0.0684,  0.0370,  0.0350,\n",
      "         0.0479,  0.0693,  0.0043, -0.0837, -0.0413, -0.0613,  0.0175, -0.0838,\n",
      "        -0.0783,  0.0449,  0.0324, -0.0806, -0.0184, -0.0009,  0.0442, -0.0859,\n",
      "         0.0041,  0.0612, -0.0055,  0.0484,  0.0627,  0.0875, -0.0698, -0.0169,\n",
      "        -0.0353,  0.0010, -0.0673, -0.0676,  0.0538,  0.0605, -0.0424,  0.0138,\n",
      "        -0.0741,  0.0073,  0.0744, -0.0136,  0.0234, -0.0270, -0.0107,  0.0248,\n",
      "         0.0753,  0.0370,  0.0145, -0.0800,  0.0859,  0.0624, -0.0469, -0.0621,\n",
      "         0.0358, -0.0818,  0.0680,  0.0053, -0.0481, -0.0190, -0.0149,  0.0364,\n",
      "         0.0367,  0.0203, -0.0417,  0.0530,  0.0271, -0.0331,  0.0527, -0.0434,\n",
      "        -0.0861,  0.0061, -0.0102,  0.0649, -0.0642, -0.0370, -0.0051, -0.0211,\n",
      "        -0.0584, -0.0331,  0.0413, -0.0741, -0.0854, -0.0849,  0.0300, -0.0145,\n",
      "         0.0723,  0.0502, -0.0649, -0.0732, -0.0442,  0.0016,  0.0821, -0.0259,\n",
      "        -0.0064, -0.0158,  0.0259, -0.0053, -0.0682,  0.0121,  0.0118,  0.0855,\n",
      "        -0.0551,  0.0336, -0.0095, -0.0246,  0.0687, -0.0527, -0.0109,  0.0772,\n",
      "        -0.0700,  0.0749,  0.0730,  0.0050, -0.0124,  0.0059,  0.0854, -0.0502,\n",
      "        -0.0321, -0.0621, -0.0574,  0.0687, -0.0244, -0.0787, -0.0241, -0.0143,\n",
      "         0.0509,  0.0571,  0.0616,  0.0728, -0.0155, -0.0539,  0.0777, -0.0503,\n",
      "         0.0232,  0.0749,  0.0383, -0.0175,  0.0863, -0.0354, -0.0375, -0.0171,\n",
      "        -0.0436, -0.0556, -0.0233, -0.0321,  0.0017, -0.0815,  0.0289,  0.0459,\n",
      "         0.0493, -0.0114, -0.0138, -0.0691, -0.0287,  0.0493, -0.0261, -0.0591,\n",
      "        -0.0632, -0.0008,  0.0259,  0.0474,  0.0140, -0.0522,  0.0581, -0.0149,\n",
      "         0.0312,  0.0429,  0.0730, -0.0397,  0.0048, -0.0733,  0.0514, -0.0447,\n",
      "        -0.0482, -0.0580, -0.0320,  0.0081, -0.0245, -0.0603,  0.0032,  0.0073,\n",
      "        -0.0839,  0.0622,  0.0163, -0.0181,  0.0552,  0.0082,  0.0274, -0.0687,\n",
      "        -0.0014, -0.0593, -0.0155, -0.0676,  0.0243, -0.0796,  0.0388, -0.0174,\n",
      "        -0.0529, -0.0218,  0.0603,  0.0171, -0.0138,  0.0073, -0.0498,  0.0367,\n",
      "         0.0238,  0.0745, -0.0238,  0.0466, -0.0779,  0.0656, -0.0515,  0.0424,\n",
      "        -0.0175,  0.0798, -0.0827, -0.0748, -0.0820,  0.0708,  0.0375, -0.0678,\n",
      "         0.0197,  0.0754, -0.0437, -0.0160,  0.0762,  0.0821, -0.0874,  0.0036,\n",
      "        -0.0631, -0.0700,  0.0244,  0.0665, -0.0196,  0.0755,  0.0603,  0.0631],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0250, -0.0167,  0.0469,  ...,  0.0695,  0.0716, -0.0084],\n",
      "        [-0.0210, -0.0845,  0.0508,  ...,  0.0172, -0.0157, -0.0607],\n",
      "        [ 0.0357,  0.0755,  0.0702,  ...,  0.0036, -0.0412, -0.0849],\n",
      "        ...,\n",
      "        [-0.0330, -0.0840, -0.0637,  ..., -0.0226,  0.0184,  0.0441],\n",
      "        [-0.0249,  0.0538,  0.0338,  ...,  0.0591, -0.0261,  0.0186],\n",
      "        [-0.0359, -0.0328, -0.0365,  ..., -0.0462,  0.0870, -0.0322]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0226, -0.0493, -0.0294,  ...,  0.0568, -0.0678,  0.0409],\n",
      "        [-0.0194,  0.0030,  0.0008,  ..., -0.0824, -0.0423,  0.0289],\n",
      "        [-0.0095,  0.0336, -0.0090,  ...,  0.0860, -0.0162, -0.0792],\n",
      "        ...,\n",
      "        [-0.0232,  0.0087,  0.0026,  ...,  0.0315, -0.0467, -0.0758],\n",
      "        [-0.0640, -0.0812, -0.0697,  ..., -0.0491,  0.0358, -0.0185],\n",
      "        [-0.0804,  0.0489, -0.0265,  ...,  0.0590, -0.0111,  0.0778]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 1.6829e-03,  2.2226e-02,  2.1230e-02, -2.0225e-02,  2.3628e-02,\n",
      "        -5.6913e-02, -5.2298e-02, -7.5430e-02,  8.3509e-02, -8.2717e-02,\n",
      "         7.7281e-02, -5.7213e-02,  2.1905e-02,  4.9542e-02,  5.4390e-02,\n",
      "        -3.4988e-02,  8.0620e-03,  2.3771e-02, -3.6237e-02, -4.4333e-02,\n",
      "         7.9389e-02, -2.8410e-02,  3.5537e-02, -2.0580e-02,  2.2073e-02,\n",
      "        -2.3810e-02,  1.8043e-02, -1.1968e-02, -1.8304e-02, -1.1343e-04,\n",
      "        -9.0844e-03, -8.3697e-02,  2.8719e-02, -7.7606e-03,  5.9745e-03,\n",
      "        -4.4433e-02,  2.3909e-02, -9.9348e-03, -6.8122e-02, -9.5149e-03,\n",
      "        -4.3246e-02,  4.6101e-02, -3.5312e-02,  7.7351e-02, -1.0777e-02,\n",
      "        -7.6397e-02, -6.6499e-02,  3.4545e-02, -8.6073e-02,  4.4230e-02,\n",
      "         6.6473e-03,  5.9869e-03,  8.1471e-02,  1.7609e-02, -7.6579e-02,\n",
      "         1.4939e-02, -5.9174e-03,  1.1148e-03,  1.2975e-02, -5.4030e-02,\n",
      "         8.8219e-02, -7.0138e-02,  8.0370e-02,  3.1439e-02, -8.1862e-02,\n",
      "        -7.9541e-03, -2.5818e-02,  2.0495e-02, -4.7093e-02, -5.3104e-02,\n",
      "        -3.7826e-02, -7.1396e-02,  3.5247e-02,  2.5204e-05, -1.7114e-02,\n",
      "        -9.8788e-03, -8.7613e-02,  1.2491e-02,  2.7087e-02, -8.2453e-02,\n",
      "         6.2252e-02,  1.8730e-02, -3.0260e-02, -6.8725e-02, -6.7336e-02,\n",
      "         1.7086e-03,  2.0763e-02,  4.6779e-02, -3.9061e-02, -8.3555e-02,\n",
      "         1.6685e-02,  5.6570e-02, -6.4934e-02,  5.7674e-02, -6.3772e-02,\n",
      "        -6.6480e-02, -5.8870e-02,  3.0893e-02,  6.4232e-02,  8.0884e-03,\n",
      "         5.5994e-02, -7.0152e-02, -2.1029e-02, -8.0428e-02,  7.8061e-02,\n",
      "        -3.3788e-02, -2.6661e-02,  2.5966e-02,  7.0239e-02, -8.2003e-02,\n",
      "        -6.1826e-03,  6.3484e-02, -4.3225e-02, -7.7005e-02, -5.2263e-02,\n",
      "        -8.7776e-02,  6.7931e-02, -1.5044e-02, -8.3363e-02,  2.1939e-03,\n",
      "         1.4366e-02, -6.2799e-02, -1.7603e-03, -8.2366e-02,  2.6765e-02,\n",
      "        -7.8949e-02,  3.1399e-02,  1.8548e-02,  5.1088e-02, -5.9820e-02,\n",
      "        -1.3863e-02,  4.6511e-02, -2.2288e-02,  8.2723e-02, -4.3734e-02,\n",
      "         1.4514e-02, -3.3519e-02, -8.4969e-02, -4.1522e-02,  6.0779e-02,\n",
      "        -5.7721e-02, -6.7255e-02,  5.1567e-02,  9.7685e-03, -6.5154e-03,\n",
      "        -1.0151e-02,  3.5516e-02,  2.4805e-02, -8.7626e-02, -6.5930e-02,\n",
      "        -3.0027e-02,  7.3275e-02, -5.5109e-02,  6.2731e-02, -7.9704e-02,\n",
      "        -8.1827e-02,  2.3130e-03, -1.0947e-02, -5.2790e-03, -2.3179e-02,\n",
      "        -1.5301e-02,  8.2314e-02, -7.1722e-02, -8.1026e-02, -5.6294e-02,\n",
      "        -4.2810e-02,  2.1921e-02,  9.1988e-03, -4.8522e-02, -7.7828e-02,\n",
      "         4.9127e-02,  3.3303e-03, -5.5064e-02, -1.5928e-02, -1.1562e-02,\n",
      "        -4.8784e-02, -4.0688e-02,  1.0406e-02,  8.4167e-02, -1.3367e-02,\n",
      "         5.1083e-02,  7.4889e-02, -2.8924e-02,  8.1713e-03,  8.3799e-03,\n",
      "         3.2575e-02, -4.9555e-02,  5.9027e-02,  3.3561e-02,  2.2245e-02,\n",
      "         4.8550e-02,  5.9633e-02, -1.7707e-02, -8.2235e-03,  4.2101e-02,\n",
      "         3.4580e-03, -8.2641e-02, -8.2921e-02,  1.9465e-02, -8.5959e-03,\n",
      "         2.1502e-02,  2.6330e-02, -1.9329e-02, -3.3333e-02, -3.8074e-02,\n",
      "         1.1669e-02,  6.6782e-02,  2.4193e-03, -4.9404e-02, -4.7765e-02,\n",
      "        -6.7357e-02,  6.8623e-02, -5.9143e-02,  5.1376e-02,  1.0791e-02,\n",
      "         6.3703e-02, -7.3423e-02,  8.0277e-02, -3.8081e-02,  1.3187e-02,\n",
      "         6.3488e-02, -3.9665e-02,  4.7651e-02, -1.0424e-02, -2.1943e-02,\n",
      "         3.1236e-02, -2.5529e-02, -2.6252e-02,  2.3735e-03,  4.4474e-03,\n",
      "         2.7474e-02, -8.7229e-04, -3.9575e-03,  8.3667e-02,  3.4387e-02,\n",
      "        -2.2518e-03,  8.1318e-02,  2.3736e-02,  7.3860e-02, -7.3120e-02,\n",
      "         4.1478e-02, -8.5719e-02,  2.6091e-02, -3.7599e-02,  3.8995e-02,\n",
      "        -8.5096e-02,  8.5876e-02, -3.5178e-02,  1.6737e-03, -8.4646e-02,\n",
      "        -1.1631e-02,  1.1039e-02,  7.6912e-02, -3.0625e-02, -6.5906e-02,\n",
      "         5.4357e-02, -1.2323e-02,  4.7270e-02,  7.5434e-02,  8.0739e-02,\n",
      "        -1.7655e-02,  4.8408e-03,  4.6423e-02,  3.2149e-02,  4.7666e-02,\n",
      "        -1.7076e-02, -2.1846e-02,  1.5688e-02,  8.2903e-02, -6.1008e-02,\n",
      "         2.8373e-02, -7.1733e-02,  3.4523e-02,  8.7914e-02, -4.5645e-02,\n",
      "         6.4156e-02,  2.1498e-02, -1.0264e-02,  4.5061e-02,  6.8840e-02,\n",
      "         5.1314e-02, -7.5629e-02,  8.6857e-02,  2.0393e-02, -1.0494e-02,\n",
      "        -5.8412e-02,  4.4609e-02, -4.4239e-04,  4.2689e-02,  6.4014e-02,\n",
      "        -4.6962e-02, -6.1005e-02, -1.4294e-02,  3.1047e-03, -2.1017e-02,\n",
      "        -3.4923e-02, -7.7885e-02,  1.4628e-02,  5.0094e-02, -8.2078e-02,\n",
      "         7.8480e-02, -3.6203e-02, -4.0432e-02, -3.8461e-02,  7.5899e-02,\n",
      "        -4.0890e-02, -7.3321e-02, -2.6359e-02,  8.5008e-02, -4.3429e-02,\n",
      "         1.3520e-02,  7.8263e-02, -8.4416e-02, -5.3931e-02, -5.9256e-02,\n",
      "        -5.9490e-02,  2.3780e-02,  3.3535e-03,  3.1227e-02, -3.1659e-02,\n",
      "         3.7648e-02, -7.0868e-02,  4.3543e-02,  5.3499e-02, -5.0877e-02,\n",
      "         5.9052e-02, -4.3550e-02,  2.5315e-02,  2.2488e-02, -1.6045e-02,\n",
      "        -5.3483e-02,  5.5262e-02,  6.6192e-02,  7.6173e-02, -3.8024e-02,\n",
      "        -7.2084e-02,  1.2413e-02,  5.7627e-02,  1.0645e-02, -5.3583e-02,\n",
      "        -7.1849e-02,  4.8557e-02,  4.2457e-02, -3.4593e-02, -8.1329e-02,\n",
      "        -5.8596e-02,  1.0128e-02, -8.0542e-02, -3.2937e-02,  9.6610e-03,\n",
      "        -6.9671e-02, -8.7308e-02, -8.9347e-03, -1.8776e-02,  1.2138e-02,\n",
      "         6.1079e-02, -1.6321e-03, -1.2148e-02, -5.8519e-02,  6.0514e-02,\n",
      "        -9.3685e-03, -1.7913e-02, -7.8385e-02,  3.1516e-02, -8.7481e-02,\n",
      "        -5.3214e-02,  3.4967e-02, -4.4310e-02,  3.3060e-02, -1.5316e-02,\n",
      "        -1.8156e-02,  2.4900e-02, -6.8585e-02, -1.4378e-02,  7.9393e-02,\n",
      "        -5.6039e-02, -8.2326e-02, -7.8384e-02, -6.2169e-02, -1.7946e-03,\n",
      "        -5.0218e-02, -4.5075e-02,  6.2637e-02,  3.1904e-02],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0830, -0.0063,  0.0420, -0.0516, -0.0495, -0.0827, -0.0388, -0.0187,\n",
      "        -0.0595, -0.0873,  0.0434, -0.0021,  0.0882, -0.0352, -0.0301, -0.0744,\n",
      "        -0.0514,  0.0309,  0.0206, -0.0562,  0.0258, -0.0527,  0.0646,  0.0107,\n",
      "        -0.0080, -0.0623, -0.0011,  0.0276, -0.0706,  0.0315, -0.0105, -0.0751,\n",
      "         0.0018,  0.0873,  0.0869, -0.0230, -0.0119,  0.0352, -0.0542,  0.0255,\n",
      "        -0.0170, -0.0279, -0.0207, -0.0047,  0.0268, -0.0255,  0.0664,  0.0397,\n",
      "        -0.0666,  0.0311, -0.0258, -0.0067,  0.0359,  0.0755,  0.0293, -0.0452,\n",
      "         0.0319,  0.0414, -0.0195,  0.0679, -0.0821,  0.0536, -0.0134,  0.0482,\n",
      "         0.0314, -0.0669, -0.0669,  0.0695,  0.0341, -0.0111, -0.0317, -0.0627,\n",
      "         0.0566, -0.0684,  0.0506, -0.0474,  0.0006,  0.0442, -0.0744,  0.0276,\n",
      "         0.0212, -0.0203, -0.0749, -0.0348, -0.0707, -0.0151,  0.0319,  0.0043,\n",
      "         0.0797, -0.0870, -0.0430,  0.0585,  0.0679,  0.0477, -0.0863, -0.0751,\n",
      "         0.0551, -0.0644, -0.0286, -0.0771, -0.0219, -0.0327, -0.0362, -0.0834,\n",
      "         0.0319, -0.0340,  0.0056,  0.0853,  0.0554, -0.0805, -0.0006, -0.0385,\n",
      "         0.0881,  0.0207,  0.0033, -0.0247,  0.0650,  0.0477, -0.0260, -0.0287,\n",
      "         0.0453, -0.0442, -0.0025,  0.0284, -0.0087, -0.0477,  0.0433, -0.0553,\n",
      "        -0.0676, -0.0847, -0.0052, -0.0648,  0.0379,  0.0078,  0.0069,  0.0623,\n",
      "         0.0300,  0.0269, -0.0589,  0.0187, -0.0518, -0.0349,  0.0712,  0.0316,\n",
      "         0.0649,  0.0320, -0.0399, -0.0674, -0.0205,  0.0356, -0.0226, -0.0005,\n",
      "         0.0069,  0.0041,  0.0317, -0.0122, -0.0689, -0.0595, -0.0751,  0.0733,\n",
      "        -0.0286,  0.0107,  0.0032, -0.0113,  0.0837, -0.0485, -0.0422,  0.0166,\n",
      "        -0.0567, -0.0705,  0.0709,  0.0266, -0.0489,  0.0532,  0.0066, -0.0809,\n",
      "         0.0492,  0.0390,  0.0124, -0.0754, -0.0755,  0.0842, -0.0285,  0.0218,\n",
      "        -0.0737, -0.0640,  0.0765,  0.0817, -0.0684,  0.0291, -0.0112, -0.0660,\n",
      "         0.0371, -0.0165, -0.0668, -0.0718,  0.0548,  0.0751,  0.0867,  0.0420,\n",
      "         0.0333, -0.0414,  0.0029, -0.0856, -0.0745,  0.0648, -0.0423, -0.0634,\n",
      "        -0.0377,  0.0045,  0.0166, -0.0156,  0.0514,  0.0623,  0.0270, -0.0379,\n",
      "        -0.0651, -0.0829,  0.0385, -0.0791, -0.0572, -0.0264,  0.0365,  0.0357,\n",
      "         0.0840, -0.0629, -0.0546, -0.0863,  0.0875,  0.0355, -0.0452,  0.0431,\n",
      "        -0.0435, -0.0137,  0.0048,  0.0602, -0.0247,  0.0680,  0.0549,  0.0545,\n",
      "        -0.0624,  0.0770,  0.0259, -0.0133,  0.0814,  0.0078,  0.0519, -0.0665,\n",
      "         0.0535, -0.0252, -0.0266,  0.0725, -0.0662, -0.0462,  0.0746, -0.0344,\n",
      "        -0.0257, -0.0502,  0.0824,  0.0072,  0.0321,  0.0475,  0.0163, -0.0708,\n",
      "         0.0338, -0.0006, -0.0491,  0.0189,  0.0489, -0.0486, -0.0833, -0.0837,\n",
      "         0.0216,  0.0295,  0.0245,  0.0404, -0.0435, -0.0195,  0.0345,  0.0150,\n",
      "        -0.0519,  0.0639, -0.0398, -0.0466, -0.0307, -0.0139,  0.0666, -0.0819,\n",
      "        -0.0876,  0.0430, -0.0642,  0.0589,  0.0761, -0.0370, -0.0018,  0.0868,\n",
      "         0.0287,  0.0388, -0.0556,  0.0735, -0.0052,  0.0450, -0.0223, -0.0125,\n",
      "         0.0814, -0.0761, -0.0120, -0.0538, -0.0831, -0.0374, -0.0438,  0.0843,\n",
      "        -0.0472,  0.0283,  0.0336, -0.0374,  0.0315,  0.0659,  0.0302, -0.0414,\n",
      "         0.0669,  0.0676, -0.0259, -0.0422, -0.0681, -0.0377, -0.0717, -0.0514,\n",
      "        -0.0704, -0.0770, -0.0653,  0.0542,  0.0711,  0.0410, -0.0164, -0.0067,\n",
      "         0.0510,  0.0662,  0.0186, -0.0427,  0.0632, -0.0863, -0.0741,  0.0218,\n",
      "         0.0587,  0.0758, -0.0159, -0.0274,  0.0532,  0.0510,  0.0751,  0.0253,\n",
      "        -0.0599, -0.0385,  0.0245,  0.0627, -0.0263,  0.0738,  0.0222, -0.0564,\n",
      "        -0.0854, -0.0120, -0.0340, -0.0360,  0.0603, -0.0686,  0.0810, -0.0515,\n",
      "         0.0579,  0.0227, -0.0458,  0.0051, -0.0644, -0.0635, -0.0839,  0.0130,\n",
      "        -0.0507,  0.0349,  0.0729,  0.0229,  0.0824,  0.0631, -0.0644, -0.0684],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0501, -0.0122,  0.0651,  ..., -0.0577,  0.0081, -0.0129],\n",
      "        [-0.0691, -0.0674,  0.0749,  ...,  0.0239,  0.0201,  0.0677],\n",
      "        [ 0.0015, -0.0768, -0.0354,  ..., -0.0720, -0.0144,  0.0837],\n",
      "        ...,\n",
      "        [ 0.0302, -0.0493,  0.0797,  ...,  0.0533,  0.0204,  0.0818],\n",
      "        [-0.0489,  0.0429, -0.0485,  ...,  0.0593,  0.0214, -0.0052],\n",
      "        [ 0.0459, -0.0827, -0.0022,  ...,  0.0511, -0.0045,  0.0729]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0351, -0.0723, -0.0279, -0.0271,  0.0484,  0.0160,  0.0578, -0.0574,\n",
      "         0.0842, -0.0123, -0.0369, -0.0715,  0.0128,  0.0172, -0.0058,  0.0835,\n",
      "         0.0086,  0.0169,  0.0174,  0.0876, -0.0120,  0.0384, -0.0466, -0.0300,\n",
      "         0.0087,  0.0486,  0.0518, -0.0304,  0.0559,  0.0044, -0.0454, -0.0104,\n",
      "        -0.0279,  0.0042, -0.0603, -0.0289,  0.0236, -0.0640, -0.0147, -0.0267,\n",
      "         0.0415, -0.0765,  0.0132,  0.0387, -0.0273,  0.0121,  0.0609, -0.0737,\n",
      "        -0.0450,  0.0881,  0.0393, -0.0698,  0.0276, -0.0246, -0.0024,  0.0872,\n",
      "         0.0412, -0.0089, -0.0445,  0.0516, -0.0249, -0.0338,  0.0644,  0.0521,\n",
      "         0.0797, -0.0259, -0.0604, -0.0467, -0.0135, -0.0190, -0.0521,  0.0533,\n",
      "         0.0342, -0.0390,  0.0606, -0.0662, -0.0322,  0.0430,  0.0237, -0.0324,\n",
      "        -0.0342,  0.0366,  0.0316, -0.0108, -0.0052, -0.0588, -0.0492, -0.0063,\n",
      "        -0.0719, -0.0189, -0.0034,  0.0058, -0.0766, -0.0739],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = GRUModel(2, 128, len(dictionary)).to(device)\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04fca513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6483\n",
      "  SICINIUS. Well, here he comes.\n",
      "  MENENIUS. Calmly, I do beseech you.\n",
      "  CORIOLANUS. Ay, as an ostler, that for th' poorest piece\n",
      "    Will bear the knave by th' volume. Th' honour'd gods\n",
      "    Keep Rome in safety, and the chairs of justice\n",
      "    Supplied with worthy men! plant love among's!\n",
      "    Throng our large temples with the shows of peace,\n",
      "    And not our streets with war!\n",
      "  FIRST SENATOR. Amen, amen!\n",
      "  MENENIUS. A noble wish.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "data = all_shakespeare.split(\"\\n\\n\")\n",
    "data = list(filter(lambda x: x, data))\n",
    "random.shuffle(data)\n",
    "\n",
    "print(len(data))\n",
    "print(data[128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9d586e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MONTJOY. You know me by my habit.\n",
      "  KING HENRY. Well then, I know thee; what shall I know of thee?\n",
      "  MONTJOY. My master's mind.\n",
      "  KING HENRY. Unfold it.\n",
      "  MONTJOY. Thus says my king. Say thou to Harry of England: Though we\n",
      "    seem'd dead we did but sleep; advantage is a better soldier than\n",
      "    rashness. Tell him we could have rebuk'd him at Harfleur, but  \n",
      "    that we thought not good to bruise an injury till it were full\n",
      "    ripe. Now we speak upon our cue, and our voice is imperial:\n",
      "    England shall repent his folly, see his weakness, and admire our\n",
      "    sufferance. Bid him therefore consider of his ransom, which must\n",
      "    proportion the losses we have borne, the subjects we have lost,\n",
      "    the disgrace we have digested; which, in weight to re-answer, his\n",
      "    pettiness would bow under. For our losses his exchequer is too\n",
      "    poor; for th' effusion of our blood, the muster of his kingdom\n",
      "    too faint a number; and for our disgrace, his own person kneeling\n",
      "    at our feet but a weak and worthless satisfaction. To this add\n",
      "    defiance; and tell him, for conclusion, he hath betrayed his\n",
      "    followers, whose condemnation is pronounc'd. So far my king and\n",
      "    master; so much my office.\n",
      "  KING HENRY. What is thy name? I know thy quality.\n",
      "  MONTJOY. Montjoy.\n",
      "  KING HENRY. Thou dost thy office fairly. Turn thee back,\n",
      "    And tell thy king I do not seek him now,\n",
      "    But could be willing to march on to Calais\n",
      "    Without impeachment; for, to say the sooth-\n",
      "    Though 'tis no wisdom to confess so much  \n",
      "    Unto an enemy of craft and vantage-\n",
      "    My people are with sickness much enfeebled;\n",
      "    My numbers lessen'd; and those few I have\n",
      "    Almost no better than so many French;\n",
      "    Who when they were in health, I tell thee, herald,\n",
      "    I thought upon one pair of English legs\n",
      "    Did march three Frenchmen. Yet forgive me, God,\n",
      "    That I do brag thus; this your air of France\n",
      "    Hath blown that vice in me; I must repent.\n",
      "    Go, therefore, tell thy master here I am;\n",
      "    My ransom is this frail and worthless trunk;\n",
      "    My army but a weak and sickly guard;\n",
      "    Yet, God before, tell him we will come on,\n",
      "    Though France himself and such another neighbour\n",
      "    Stand in our way. There's for thy labour, Montjoy.\n",
      "    Go, bid thy master well advise himself.\n",
      "    If we may pass, we will; if we be hind'red,\n",
      "    We shall your tawny ground with your red blood\n",
      "    Discolour; and so, Montjoy, fare you well.\n",
      "    The sum of all our answer is but this:  \n",
      "    We would not seek a battle as we are;\n",
      "    Nor as we are, we say, we will not shun it.\n",
      "    So tell your master.\n",
      "  MONTJOY. I shall deliver so. Thanks to your Highness.     Exit\n",
      "  GLOUCESTER. I hope they will not come upon us now.\n",
      "  KING HENRY. We are in God's hand, brother, not in theirs.\n",
      "    March to the bridge, it now draws toward night;\n",
      "    Beyond the river we'll encamp ourselves,\n",
      "    And on to-morrow bid them march away.                 Exeunt\n",
      "\n",
      "  CYMBELINE. Again! and bring me word how 'tis with her.\n",
      "                                               Exit an attendant\n",
      "    A fever with the absence of her son;\n",
      "    A madness, of which her life's in danger. Heavens,\n",
      "    How deeply you at once do touch me! Imogen,\n",
      "    The great part of my comfort, gone; my queen\n",
      "    Upon a desperate bed, and in a time\n",
      "    When fearful wars point at me; her son gone,\n",
      "    So needful for this present. It strikes me past\n",
      "    The hope of comfort. But for thee, fellow,\n",
      "    Who needs must know of her departure and\n",
      "    Dost seem so ignorant, we'll enforce it from thee\n",
      "    By a sharp torture.\n",
      "  PISANIO. Sir, my life is yours;\n",
      "    I humbly set it at your will; but for my mistress,\n",
      "    I nothing know where she remains, why gone,\n",
      "    Nor when she purposes return. Beseech your Highness,  \n",
      "    Hold me your loyal servant.\n",
      "  LORD. Good my liege,\n",
      "    The day that she was missing he was here.\n",
      "    I dare be bound he's true and shall perform\n",
      "    All parts of his subjection loyally. For Cloten,\n",
      "    There wants no diligence in seeking him,\n",
      "    And will no doubt be found.\n",
      "  CYMBELINE. The time is troublesome.\n",
      "    [To PISANIO] We'll slip you for a season; but our jealousy\n",
      "    Does yet depend.\n",
      "  LORD. So please your Majesty,\n",
      "    The Roman legions, all from Gallia drawn,\n",
      "    Are landed on your coast, with a supply\n",
      "    Of Roman gentlemen by the Senate sent.\n",
      "  CYMBELINE. Now for the counsel of my son and queen!\n",
      "    I am amaz'd with matter.\n",
      "  LORD. Good my liege,\n",
      "    Your preparation can affront no less\n",
      "    Than what you hear of. Come more, for more you're ready.\n",
      "    The want is but to put those pow'rs in motion  \n",
      "    That long to move.\n",
      "  CYMBELINE. I thank you. Let's withdraw,\n",
      "    And meet the time as it seeks us. We fear not\n",
      "    What can from Italy annoy us; but\n",
      "    We grieve at chances here. Away!      Exeunt all but PISANIO\n",
      "  PISANIO. I heard no letter from my master since\n",
      "    I wrote him Imogen was slain. 'Tis strange.\n",
      "    Nor hear I from my mistress, who did promise\n",
      "    To yield me often tidings. Neither know\n",
      "    What is betid to Cloten, but remain\n",
      "    Perplex'd in all. The heavens still must work.\n",
      "    Wherein I am false I am honest; not true, to be true.\n",
      "    These present wars shall find I love my country,\n",
      "    Even to the note o' th' King, or I'll fall in them.\n",
      "    All other doubts, by time let them be clear'd:\n",
      "    Fortune brings in some boats that are not steer'd.      Exit\n"
     ]
    }
   ],
   "source": [
    "train = [data[i] for i in range(len(data)) if i % 10 != 0]\n",
    "test = [data[i] for i in range(len(data)) if i % 10 == 0]\n",
    "\n",
    "print(train[-5])\n",
    "print(\"\")\n",
    "print(test[-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59b3fd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U=Wtabcy[u3Gc7MOxNSAj?]YN[X_|iEnCp7,\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate(model, len_limit):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        result = \"\"\n",
    "        state = model.zero_state(None).to(device)\n",
    "        x = \"<start>\"\n",
    "        while len(result) < len_limit:\n",
    "            x = torch.tensor([sym2idx[x]]).to(device)\n",
    "            y, state = model(x, state)\n",
    "            y = y[0].cpu().numpy()\n",
    "            y = np.exp(y)\n",
    "            y /= np.sum(y)\n",
    "            x = dictionary[np.random.choice(y.shape[0], p = y)]\n",
    "            if x in [\"<start>\", \"<end>\", \"<empty>\"]:\n",
    "                break\n",
    "            result += x\n",
    "        return result\n",
    "\n",
    "print(generate(model, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea79e5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [00:19<00:00,  5.09it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:02<00:00, 34.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 4.49801 3.56978\n",
      "%B`Z;,os!0EG@HN<DR`CO--]\"q`Hs;Vd6yy2Wtw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [00:19<00:00,  5.06it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:02<00:00, 33.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 3.33407 2.32542\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████████████████████████▍               | 63/100 [00:08<00:05,  7.38it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m total_count\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 57\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m test_epoch(test[:\u001b[38;5;241m100\u001b[39m], model)\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m loss: \u001b[39m\u001b[38;5;132;01m{:.5f}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{:.5f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i, train_loss, test_loss))\n",
      "Cell \u001b[0;32mIn[8], line 38\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(data, model)\u001b[0m\n\u001b[1;32m     36\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     37\u001b[0m     total_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 38\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m total_count\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "def iterate_batches(data, batch_size, device):\n",
    "    x, y, max_len = [], [], 0\n",
    "    for k in tqdm.tqdm(range(len(data))):\n",
    "        item = data[k]\n",
    "        x.append([sym2idx[sym] for sym in [\"<start>\"] + list(item)])\n",
    "        y.append([sym2idx[sym] for sym in list(item) + [\"<end>\"]])\n",
    "        max_len = max(max_len, len(x[-1]))\n",
    "        if len(x) == batch_size or k + 1 == len(data):\n",
    "            for i in range(len(x)):\n",
    "                x[i] = x[i] + [sym2idx[\"<empty>\"] for _ in range(max_len - len(x[i]))]\n",
    "                y[i] = y[i] + [sym2idx[\"<empty>\"] for _ in range(max_len - len(y[i]))]\n",
    "            x = torch.tensor(x).to(device)\n",
    "            y = torch.tensor(y).to(device)\n",
    "            yield x, y\n",
    "            x, y, max_len = [], [], 0\n",
    "        \n",
    "\n",
    "def train_epoch(data, model):\n",
    "    model.train()\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    optimizer = torch.optim.AdamW(model.parameters())\n",
    "    total_loss, total_count = 0.0, 1e-38\n",
    "    random.shuffle(data)\n",
    "    for inputs, answers in iterate_batches(data, 64, device):\n",
    "        optimizer.zero_grad()\n",
    "        state = model.zero_state(inputs.shape[0]).to(device)\n",
    "        #print(inputs.shape)\n",
    "        #print(answers.shape)\n",
    "        outputs, state = model(inputs, state)\n",
    "        outputs = outputs.transpose(1, 2)\n",
    "        #print(outputs.shape)\n",
    "        loss = loss_function(outputs, answers)\n",
    "        total_loss += (loss.item() * inputs.shape[0])\n",
    "        total_count += inputs.shape[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss / total_count\n",
    "\n",
    "def test_epoch(data, model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        loss_function = torch.nn.CrossEntropyLoss()\n",
    "        total_loss, total_count = 0.0, 1e-38\n",
    "        for inputs, answers in iterate_batches(data, 64, device):\n",
    "            state = model.zero_state(inputs.shape[0]).to(device)\n",
    "            outputs, state = model(inputs, state)\n",
    "            outputs = outputs.transpose(1, 2)\n",
    "            loss = loss_function(outputs, answers)\n",
    "            total_loss += (loss.item() * inputs.shape[0])\n",
    "            total_count += inputs.shape[0]\n",
    "        return total_loss / total_count\n",
    "\n",
    "for i in range(10):\n",
    "    train_loss = train_epoch(train[:100], model)\n",
    "    test_loss = test_epoch(test[:100], model)\n",
    "    print(\"Epoch {} loss: {:.5f} {:.5f}\".format(i, train_loss, test_loss))\n",
    "    print(generate(model, 1000))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2416d730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
