{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "class RNNModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, dict_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dict_size = dict_size\n",
    "        self.embeddings = torch.nn.Embedding(dict_size, hidden_size)\n",
    "        self.wh = torch.nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.wy = torch.nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.uh = torch.nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.bh = torch.nn.Parameter(torch.randn(hidden_size))\n",
    "        self.by = torch.nn.Parameter(torch.randn(hidden_size))\n",
    "        self.projection = torch.nn.Linear(hidden_size, dict_size)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        x = self.embeddings(x)\n",
    "        h = torch.sigmoid(self.wh(x) + self.uh(h) + self.bh)\n",
    "        y = self.projection(torch.sigmoid(self.wy(h) + self.by))\n",
    "        return y, h\n",
    "\n",
    "    def zero_state(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5458199\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "all_shakespeare = requests.get(\"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\").content.decode()\n",
    "print(len(all_shakespeare))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P', 'D', 'B', 'Z', 'T', ';', 'W', '\\n', 'S', '>', 'p', 'u', 'g', 'd', 'E', '|', '(', 'n', 'O', '9', '6', '/', '`', ')', 't', 'V', 'q', 'A', 'm', '1', ':', 'j', 'w', 'h', '-', '*', '@', 'R', 'Y', 'N', 'c', \"'\", '?', ',', '%', 'f', 'i', 'a', 'I', '!', ']', '7', 's', 'G', 'F', '2', '[', '5', 'M', 'X', 'k', 'C', '.', 'x', 'r', 'y', 'z', 'Q', '_', '8', 'J', 'L', 'U', '\"', 'K', '&', 'b', ' ', '=', '0', '~', '4', 'v', 'e', 'H', '<', '}', '#', 'o', '3', 'l', '<start>', '<end>', '<empty>']\n",
      "94\n",
      "{'P': 0, 'D': 1, 'B': 2, 'Z': 3, 'T': 4, ';': 5, 'W': 6, '\\n': 7, 'S': 8, '>': 9, 'p': 10, 'u': 11, 'g': 12, 'd': 13, 'E': 14, '|': 15, '(': 16, 'n': 17, 'O': 18, '9': 19, '6': 20, '/': 21, '`': 22, ')': 23, 't': 24, 'V': 25, 'q': 26, 'A': 27, 'm': 28, '1': 29, ':': 30, 'j': 31, 'w': 32, 'h': 33, '-': 34, '*': 35, '@': 36, 'R': 37, 'Y': 38, 'N': 39, 'c': 40, \"'\": 41, '?': 42, ',': 43, '%': 44, 'f': 45, 'i': 46, 'a': 47, 'I': 48, '!': 49, ']': 50, '7': 51, 's': 52, 'G': 53, 'F': 54, '2': 55, '[': 56, '5': 57, 'M': 58, 'X': 59, 'k': 60, 'C': 61, '.': 62, 'x': 63, 'r': 64, 'y': 65, 'z': 66, 'Q': 67, '_': 68, '8': 69, 'J': 70, 'L': 71, 'U': 72, '\"': 73, 'K': 74, '&': 75, 'b': 76, ' ': 77, '=': 78, '0': 79, '~': 80, '4': 81, 'v': 82, 'e': 83, 'H': 84, '<': 85, '}': 86, '#': 87, 'o': 88, '3': 89, 'l': 90, '<start>': 91, '<end>': 92, '<empty>': 93}\n"
     ]
    }
   ],
   "source": [
    "dictionary = list(set(all_shakespeare))\n",
    "dictionary.append(\"<start>\")\n",
    "dictionary.append(\"<end>\")\n",
    "dictionary.append(\"<empty>\")\n",
    "print(dictionary)\n",
    "print(len(dictionary))\n",
    "\n",
    "sym2idx = {s: i for i, s in enumerate(dictionary)}\n",
    "print(sym2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 0.8450,  0.4010, -2.1780, -0.1664, -0.4047, -0.7855,  1.5931, -0.5891,\n",
      "         0.6570,  0.1699, -1.3531, -0.4122,  1.1594, -0.6741, -1.5409, -0.2923,\n",
      "         0.8097,  0.1572, -1.3926,  1.0935,  0.5900, -0.7343, -1.0554, -0.0265,\n",
      "         0.4737,  0.5921, -1.2053, -0.1082,  0.7025,  0.4492, -0.9323,  1.1177,\n",
      "         0.5482, -0.6988,  0.4176,  0.5328,  1.0138, -0.5300,  0.2484,  0.1550,\n",
      "        -0.2713, -0.9858,  0.9586,  0.5857,  0.8949, -1.1385,  0.6538,  0.1927,\n",
      "        -1.0815,  0.7078,  0.6650,  0.2809, -0.8523,  0.3628, -0.2790, -0.1556,\n",
      "         1.4167, -0.1194,  0.6041,  0.6225,  0.6362, -0.6690,  1.2722, -0.0845,\n",
      "        -0.6244, -0.7992, -1.5093, -0.2532, -0.6413,  0.9036,  1.0816,  0.9328,\n",
      "         1.4068, -0.0289,  0.7068,  0.4110,  0.2703,  0.0034,  0.2026, -0.6429,\n",
      "        -0.0257,  1.2472,  1.1970, -0.1720, -0.8664, -0.5702, -0.0184, -0.7364,\n",
      "        -0.8061,  0.5111,  1.0159, -0.7839, -0.0441,  0.7904,  1.3348, -0.1857,\n",
      "        -0.3873, -0.3075, -0.9106,  0.7447, -0.2896,  1.5024, -0.9562, -0.3076,\n",
      "         0.1458, -0.6979, -1.8906,  2.5549,  0.9964,  0.6687, -0.4380, -0.4989,\n",
      "        -0.7904,  0.5886,  1.0637, -0.0471,  0.5014, -0.4951,  0.9853,  1.4068,\n",
      "        -0.5307,  0.2042,  0.5513,  1.6465, -2.0366, -0.8293, -0.0507,  0.6886],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0246, -0.4991,  0.1696, -0.5585, -0.2035,  0.0904, -1.3663,  0.7993,\n",
      "         0.8717, -1.2156, -0.1816, -1.2580, -1.0121,  0.9016, -0.4907, -0.5243,\n",
      "         0.1378,  1.5890, -1.4091, -1.3498,  1.2967,  0.2398, -1.5577,  0.4991,\n",
      "        -0.8149, -0.4674,  2.2851,  0.8962,  0.1612,  1.3717, -1.6020,  0.8354,\n",
      "         1.7260,  0.5133, -1.9521, -0.5779,  2.0528,  2.0586,  0.8347,  0.3366,\n",
      "         0.1296,  0.3274, -0.0251,  0.0395,  0.1924, -1.3679,  1.0085, -0.8412,\n",
      "        -0.8598, -0.5125, -0.7982,  0.3497,  1.6065, -1.0331,  0.3805, -0.3059,\n",
      "         0.7285, -1.4608, -1.3451,  0.4665, -2.0096, -0.3073, -0.4130,  0.3541,\n",
      "         0.5314, -0.1641, -0.1416, -0.3495,  0.9990, -1.0244,  1.0129, -0.0300,\n",
      "         0.8337,  0.1767, -1.0301, -0.0947, -0.1755,  1.1194,  2.1284,  0.1587,\n",
      "         0.3038, -1.0874,  0.4241, -1.1264, -0.4667,  0.5471, -0.6714, -0.1133,\n",
      "        -0.5857,  1.6860, -0.2073,  0.0130, -2.0062, -0.7013, -0.0839,  0.7772,\n",
      "         1.2334, -0.3146,  0.4960,  0.9234, -0.0130, -0.4904, -0.1717,  0.0056,\n",
      "        -2.0922,  0.6734, -0.5234, -0.3193,  0.5065,  0.6568,  0.9106,  1.2103,\n",
      "        -0.6333,  0.5467,  0.9621,  1.3195,  0.8546, -0.3692,  0.3412, -0.6426,\n",
      "         0.7168,  0.0811, -0.0132, -1.4010,  0.0610,  0.0493, -0.2579, -0.3122],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.0133, -0.0219, -2.3323,  ...,  0.9825, -0.6441, -1.7603],\n",
      "        [-1.2197,  0.8108,  0.3419,  ..., -1.2675, -1.3368, -2.0001],\n",
      "        [ 0.8789, -0.2610,  0.4201,  ...,  0.1190, -0.9137,  0.5868],\n",
      "        ...,\n",
      "        [-0.5352,  0.7766,  0.5018,  ...,  0.3178,  0.0963,  0.6423],\n",
      "        [-0.1697, -1.5218, -0.8754,  ..., -0.0724,  0.4747,  1.9200],\n",
      "        [ 0.3536, -1.6718, -0.8113,  ...,  1.2395, -1.5897, -0.0641]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0193,  0.0679,  0.0157,  ..., -0.0806,  0.0585,  0.0362],\n",
      "        [-0.0355, -0.0843,  0.0137,  ...,  0.0197,  0.0214, -0.0122],\n",
      "        [ 0.0382,  0.0148, -0.0656,  ..., -0.0472, -0.0386, -0.0529],\n",
      "        ...,\n",
      "        [-0.0405,  0.0054, -0.0309,  ...,  0.0652, -0.0642,  0.0045],\n",
      "        [-0.0288, -0.0112,  0.0779,  ..., -0.0274,  0.0522,  0.0251],\n",
      "        [-0.0674, -0.0776,  0.0881,  ..., -0.0502,  0.0796, -0.0589]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0699,  0.0205,  0.0081,  ...,  0.0212,  0.0680,  0.0157],\n",
      "        [ 0.0800,  0.0335,  0.0178,  ...,  0.0528,  0.0382,  0.0613],\n",
      "        [-0.0723,  0.0530,  0.0861,  ...,  0.0021, -0.0202, -0.0406],\n",
      "        ...,\n",
      "        [-0.0865, -0.0337, -0.0097,  ..., -0.0604, -0.0240, -0.0227],\n",
      "        [-0.0641,  0.0455,  0.0608,  ..., -0.0059, -0.0188, -0.0385],\n",
      "        [-0.0203,  0.0244, -0.0615,  ...,  0.0530,  0.0807,  0.0027]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0814,  0.0654, -0.0833,  ...,  0.0679, -0.0662,  0.0737],\n",
      "        [-0.0761, -0.0104,  0.0075,  ...,  0.0616, -0.0769, -0.0394],\n",
      "        [-0.0246,  0.0421,  0.0497,  ...,  0.0196,  0.0611, -0.0606],\n",
      "        ...,\n",
      "        [-0.0507, -0.0265,  0.0828,  ..., -0.0395, -0.0632,  0.0805],\n",
      "        [ 0.0664,  0.0830, -0.0773,  ...,  0.0309,  0.0061, -0.0668],\n",
      "        [ 0.0457, -0.0772,  0.0608,  ..., -0.0804,  0.0328, -0.0521]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0285,  0.0782, -0.0011,  ...,  0.0137,  0.0804, -0.0046],\n",
      "        [-0.0430, -0.0149,  0.0465,  ..., -0.0239,  0.0235,  0.0104],\n",
      "        [-0.0668, -0.0735, -0.0025,  ...,  0.0647,  0.0445,  0.0759],\n",
      "        ...,\n",
      "        [-0.0882,  0.0589, -0.0735,  ...,  0.0121, -0.0087,  0.0775],\n",
      "        [ 0.0306,  0.0410,  0.0396,  ..., -0.0610, -0.0494,  0.0507],\n",
      "        [-0.0239, -0.0058, -0.0634,  ..., -0.0803, -0.0035,  0.0091]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0856,  0.0310, -0.0120,  0.0253,  0.0172, -0.0463,  0.0281,  0.0304,\n",
      "        -0.0497, -0.0693,  0.0865, -0.0361,  0.0092,  0.0327, -0.0205,  0.0182,\n",
      "         0.0696, -0.0246, -0.0548, -0.0764, -0.0529, -0.0735,  0.0518,  0.0349,\n",
      "        -0.0076, -0.0810, -0.0179, -0.0467, -0.0659,  0.0629, -0.0745,  0.0461,\n",
      "         0.0761,  0.0423,  0.0080, -0.0384, -0.0133,  0.0670, -0.0854, -0.0248,\n",
      "         0.0598,  0.0480, -0.0129, -0.0159,  0.0505, -0.0586,  0.0366,  0.0194,\n",
      "         0.0682,  0.0873, -0.0008,  0.0714,  0.0085,  0.0086,  0.0200, -0.0438,\n",
      "        -0.0339,  0.0369,  0.0520,  0.0567,  0.0639, -0.0733, -0.0253, -0.0080,\n",
      "         0.0532, -0.0261, -0.0299,  0.0509, -0.0143, -0.0054, -0.0457, -0.0411,\n",
      "         0.0875, -0.0515, -0.0157,  0.0265,  0.0852, -0.0372,  0.0552,  0.0852,\n",
      "         0.0747,  0.0522,  0.0703,  0.0125, -0.0362, -0.0448,  0.0010,  0.0729,\n",
      "         0.0720, -0.0179, -0.0414, -0.0559, -0.0757, -0.0629], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = RNNModel(128, len(dictionary)).to(device)\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6483\n",
      "  SICINIUS. Well, here he comes.\n",
      "  MENENIUS. Calmly, I do beseech you.\n",
      "  CORIOLANUS. Ay, as an ostler, that for th' poorest piece\n",
      "    Will bear the knave by th' volume. Th' honour'd gods\n",
      "    Keep Rome in safety, and the chairs of justice\n",
      "    Supplied with worthy men! plant love among's!\n",
      "    Throng our large temples with the shows of peace,\n",
      "    And not our streets with war!\n",
      "  FIRST SENATOR. Amen, amen!\n",
      "  MENENIUS. A noble wish.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "data = all_shakespeare.split(\"\\n\\n\")\n",
    "data = list(filter(lambda x: x, data))\n",
    "random.shuffle(data)\n",
    "\n",
    "print(len(data))\n",
    "print(data[128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MONTJOY. You know me by my habit.\n",
      "  KING HENRY. Well then, I know thee; what shall I know of thee?\n",
      "  MONTJOY. My master's mind.\n",
      "  KING HENRY. Unfold it.\n",
      "  MONTJOY. Thus says my king. Say thou to Harry of England: Though we\n",
      "    seem'd dead we did but sleep; advantage is a better soldier than\n",
      "    rashness. Tell him we could have rebuk'd him at Harfleur, but  \n",
      "    that we thought not good to bruise an injury till it were full\n",
      "    ripe. Now we speak upon our cue, and our voice is imperial:\n",
      "    England shall repent his folly, see his weakness, and admire our\n",
      "    sufferance. Bid him therefore consider of his ransom, which must\n",
      "    proportion the losses we have borne, the subjects we have lost,\n",
      "    the disgrace we have digested; which, in weight to re-answer, his\n",
      "    pettiness would bow under. For our losses his exchequer is too\n",
      "    poor; for th' effusion of our blood, the muster of his kingdom\n",
      "    too faint a number; and for our disgrace, his own person kneeling\n",
      "    at our feet but a weak and worthless satisfaction. To this add\n",
      "    defiance; and tell him, for conclusion, he hath betrayed his\n",
      "    followers, whose condemnation is pronounc'd. So far my king and\n",
      "    master; so much my office.\n",
      "  KING HENRY. What is thy name? I know thy quality.\n",
      "  MONTJOY. Montjoy.\n",
      "  KING HENRY. Thou dost thy office fairly. Turn thee back,\n",
      "    And tell thy king I do not seek him now,\n",
      "    But could be willing to march on to Calais\n",
      "    Without impeachment; for, to say the sooth-\n",
      "    Though 'tis no wisdom to confess so much  \n",
      "    Unto an enemy of craft and vantage-\n",
      "    My people are with sickness much enfeebled;\n",
      "    My numbers lessen'd; and those few I have\n",
      "    Almost no better than so many French;\n",
      "    Who when they were in health, I tell thee, herald,\n",
      "    I thought upon one pair of English legs\n",
      "    Did march three Frenchmen. Yet forgive me, God,\n",
      "    That I do brag thus; this your air of France\n",
      "    Hath blown that vice in me; I must repent.\n",
      "    Go, therefore, tell thy master here I am;\n",
      "    My ransom is this frail and worthless trunk;\n",
      "    My army but a weak and sickly guard;\n",
      "    Yet, God before, tell him we will come on,\n",
      "    Though France himself and such another neighbour\n",
      "    Stand in our way. There's for thy labour, Montjoy.\n",
      "    Go, bid thy master well advise himself.\n",
      "    If we may pass, we will; if we be hind'red,\n",
      "    We shall your tawny ground with your red blood\n",
      "    Discolour; and so, Montjoy, fare you well.\n",
      "    The sum of all our answer is but this:  \n",
      "    We would not seek a battle as we are;\n",
      "    Nor as we are, we say, we will not shun it.\n",
      "    So tell your master.\n",
      "  MONTJOY. I shall deliver so. Thanks to your Highness.     Exit\n",
      "  GLOUCESTER. I hope they will not come upon us now.\n",
      "  KING HENRY. We are in God's hand, brother, not in theirs.\n",
      "    March to the bridge, it now draws toward night;\n",
      "    Beyond the river we'll encamp ourselves,\n",
      "    And on to-morrow bid them march away.                 Exeunt\n",
      "\n",
      "  CYMBELINE. Again! and bring me word how 'tis with her.\n",
      "                                               Exit an attendant\n",
      "    A fever with the absence of her son;\n",
      "    A madness, of which her life's in danger. Heavens,\n",
      "    How deeply you at once do touch me! Imogen,\n",
      "    The great part of my comfort, gone; my queen\n",
      "    Upon a desperate bed, and in a time\n",
      "    When fearful wars point at me; her son gone,\n",
      "    So needful for this present. It strikes me past\n",
      "    The hope of comfort. But for thee, fellow,\n",
      "    Who needs must know of her departure and\n",
      "    Dost seem so ignorant, we'll enforce it from thee\n",
      "    By a sharp torture.\n",
      "  PISANIO. Sir, my life is yours;\n",
      "    I humbly set it at your will; but for my mistress,\n",
      "    I nothing know where she remains, why gone,\n",
      "    Nor when she purposes return. Beseech your Highness,  \n",
      "    Hold me your loyal servant.\n",
      "  LORD. Good my liege,\n",
      "    The day that she was missing he was here.\n",
      "    I dare be bound he's true and shall perform\n",
      "    All parts of his subjection loyally. For Cloten,\n",
      "    There wants no diligence in seeking him,\n",
      "    And will no doubt be found.\n",
      "  CYMBELINE. The time is troublesome.\n",
      "    [To PISANIO] We'll slip you for a season; but our jealousy\n",
      "    Does yet depend.\n",
      "  LORD. So please your Majesty,\n",
      "    The Roman legions, all from Gallia drawn,\n",
      "    Are landed on your coast, with a supply\n",
      "    Of Roman gentlemen by the Senate sent.\n",
      "  CYMBELINE. Now for the counsel of my son and queen!\n",
      "    I am amaz'd with matter.\n",
      "  LORD. Good my liege,\n",
      "    Your preparation can affront no less\n",
      "    Than what you hear of. Come more, for more you're ready.\n",
      "    The want is but to put those pow'rs in motion  \n",
      "    That long to move.\n",
      "  CYMBELINE. I thank you. Let's withdraw,\n",
      "    And meet the time as it seeks us. We fear not\n",
      "    What can from Italy annoy us; but\n",
      "    We grieve at chances here. Away!      Exeunt all but PISANIO\n",
      "  PISANIO. I heard no letter from my master since\n",
      "    I wrote him Imogen was slain. 'Tis strange.\n",
      "    Nor hear I from my mistress, who did promise\n",
      "    To yield me often tidings. Neither know\n",
      "    What is betid to Cloten, but remain\n",
      "    Perplex'd in all. The heavens still must work.\n",
      "    Wherein I am false I am honest; not true, to be true.\n",
      "    These present wars shall find I love my country,\n",
      "    Even to the note o' th' King, or I'll fall in them.\n",
      "    All other doubts, by time let them be clear'd:\n",
      "    Fortune brings in some boats that are not steer'd.      Exit\n"
     ]
    }
   ],
   "source": [
    "train = [data[i] for i in range(len(data)) if i % 10 != 0]\n",
    "test = [data[i] for i in range(len(data)) if i % 10 == 0]\n",
    "\n",
    "print(train[-5])\n",
    "print(\"\")\n",
    "print(test[-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1xQ9xGVHq(C.-H4\"4R83&peAo,G_#Z\n",
      "@>L(n\"d46I)k!FVb#A\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate(model, len_limit):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        result = \"\"\n",
    "        state = model.zero_state(1).to(device)\n",
    "        x = \"<start>\"\n",
    "        while len(result) < len_limit:\n",
    "            x = torch.tensor(sym2idx[x]).to(device)\n",
    "            y, state = model(x, state)\n",
    "            y = y[0].cpu().numpy()\n",
    "            y = np.exp(y)\n",
    "            y /= np.sum(y)\n",
    "            x = dictionary[np.random.choice(y.shape[0], p = y)]\n",
    "            if x in [\"<start>\", \"<end>\", \"<empty>\"]:\n",
    "                break\n",
    "            result += x\n",
    "        return result\n",
    "\n",
    "print(generate(model, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5834/5834 [05:04<00:00, 19.14it/s]\n",
      "100%|██████████| 649/649 [00:09<00:00, 67.23it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 1.20524 0.55670\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5834/5834 [05:08<00:00, 18.93it/s]\n",
      "100%|██████████| 649/649 [00:09<00:00, 67.26it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.36456 0.37489\n",
      "n kOa.\n",
      "h\n",
      "\n",
      "Det' a DeI\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5834/5834 [05:06<00:00, 19.02it/s]\n",
      "100%|██████████| 649/649 [00:09<00:00, 67.20it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 loss: 0.30614 0.30822\n",
      " me Pd\n",
      " ear gem fism Tour. mor  tI   satesryhunet,. UWwOT\n",
      "Oyesnedrr   ikdlitive wikK! heWonee  , onlIw ;hiteuwCAthD;  I S  sto Pern sRE,  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5834/5834 [05:12<00:00, 18.67it/s]\n",
      "100%|██████████| 649/649 [00:09<00:00, 67.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 loss: 0.25233 0.27135\n",
      " dinl.\n",
      "    Tirimh me thand's avithes or danteas lre's tharmend fou thimeor jat nen thee Gharlrroref s;ungher neeerpares maye fardreu Rayollld omh reit tivewe lo siwid enir, lon to he wiy the meme mal'g heer]. LO.-ACIOEVunt wo theeserraaetindendclouce diy mideethormar  Cichheile benh cor brendolit ily buho fadb. Aot to mordend hou  ser laco ath bicatra norrenstt igoranth; savesy coos shetre thes pised deyeko ponmeuimd 'mw, Ayoiws wan winse ang cadT \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5834/5834 [05:04<00:00, 19.19it/s]\n",
      "100%|██████████| 649/649 [00:09<00:00, 67.06it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 loss: 0.23435 0.25567\n",
      " pang kke shek trolccony ir preathsy feclien thor;?\n",
      "  AR woy wiser, aHibuh,\n",
      "   AVPAd Gilte at i yulis, meard\n",
      "    IW Pwiken ilccort me yous; maod or tid, i,\n",
      "      We gand be hole hall at fere, thim; ow sherise's, trome Leyentayd manget an wind ato, to perestouch nom an halk se rind wonot ppuald, it ar noss meast Jowt ik\n",
      "         may thad diinir bleat cont:, ,\n",
      "         .\n",
      "  mosas oanmit.\n",
      "  Cant memlich? Anlcoven'em hito the hou; kirus serree irs enlelce I wiln soas ath henone hrath fore Of gou to hat-.\n",
      "  An to ad . I 's mated thu? KUUFES I Shaget Moof gote, ru! Dlay mpot;\n",
      "       ster toud thow thanteine luprit's;\n",
      "   Null urlfig wor yis yoont      Yous bond if dedthenr yo fon wir\n",
      "                            Thebecun gofribith meparnid it lot\n",
      "s ide ser roEs on seer ir itins bayco, come! millwes to sour for's.        ho woans afauvy thor\n",
      "   Gnemdirg. EX Dy Lovied gothe sey llame cold chichcireo'gs weat mores sfevey tharchgrarnemnhereen sho de?\n",
      "  Wame derom the bleshare nat coisent hem'y, and\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5834/5834 [05:04<00:00, 19.14it/s]\n",
      "100%|██████████| 649/649 [00:09<00:00, 67.16it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 loss: 0.22047 0.24698\n",
      "     Whou sen!\n",
      "  SYA. Brptoret hit wate I for sone haide fy! sir maect pely\n",
      "    wit whausce hoonond a fian sato hik When to no woic, ang Maliny erghe, ing\n",
      "    Whang thave blead forve sheper. Adhboce thiid pon corly lo; stace as bepon wish she in atry aln, coach mus to nes lispe-ben a for the hale! Bumellabe ang homcetrey me sparenenglor geEnis, meoter Amind\n",
      "      ansgatl,\n",
      "   [AMRHORARD. CIE(R  EDE Jacrovothmine dini the turercrerittous ib's, my be\n",
      "            Th, the,\n",
      "     Bother thatt mout'dom he is tpims owang Cham gatrir thI. Whid thrleek one. ONUENRSD I. Thild arl muth ulener afe mo are ave poun! Andy dim hy so ige mat hy\n",
      "    Thid chat covand hirtip'es, cuit buveshy paer coud lady or prele be it titich, of the Mest\n",
      "     \n",
      "   Nive bonofruventess.   Swarm twere\n",
      "    I noption facdele of As, yering mism\n",
      "  Tleoveras win bestinorly,  [ANCCIANVAUR. SI' Siten tall me fouge- you will as- the, my\n",
      "     To mapodnpeat ba. I ar swour, satles,\n",
      "     ske.\n",
      "  Bu by goshcour hint thath's to sark epten.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5834/5834 [05:04<00:00, 19.19it/s]\n",
      "100%|██████████| 649/649 [00:09<00:00, 67.75it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 loss: 0.21618 0.24179\n",
      " Othentlest thy\n",
      "    Senchckebou,\n",
      "  \n",
      "  If sit dim, jecc! O He thal. ATh.\n",
      "  ARURHDENUMEDIS DOCTEUH ORD\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5834/5834 [04:58<00:00, 19.57it/s]\n",
      "100%|██████████| 649/649 [00:09<00:00, 67.53it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 loss: 0.21398 0.23767\n",
      " Ove wirE. Of my Gigramt me Lean,\n",
      "    This bry I the for\n",
      "    Mes, lennot thit sele.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5834/5834 [04:57<00:00, 19.64it/s]\n",
      "100%|██████████| 649/649 [00:09<00:00, 67.51it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 loss: 0.21068 0.23402\n",
      "ANTTEMOPCOTUOd elfwono dear sut'd befire ble shat! Fray mrey\n",
      "                whis. Werst\n",
      "    To lil she hern hion 'Dentere thous stof GROS. Who\n",
      "      Ind FAt and hen of hur nees hee sert is. Conl and rorck\n",
      "   And 'nfich wive the and ip they,\n",
      "    Whell,\n",
      "       [Ist, Hesseaush hend lefcee I the, gotcowle cowelo wour buts batcaon,\n",
      "    To and;\n",
      "    Hatan afir'd nod                  [And ofuse\n",
      "    my weass conge\n",
      "    Loce wors sure ciood.\n",
      "  \n",
      "TOLD EURSHIRCOGIBETIOG. Whell of and mamerese my my\n",
      "    Vowe a stiin?\n",
      "  ANCAGRIAS\n",
      "  SIGKORSI.      you beartwerscere and do whige to strece,\n",
      "    Sire his retly thos her pravinter mave,\n",
      "    foung wore thull I af bepithad I onfoy me.\n",
      "  Wil-hit; I comslos chere? A spar you shighim dover;\n",
      "    hind Sary hove if in mars bay senght fnot my\n",
      "     Ame\n",
      "    ERIB. Ot oscey no, and heno, as my thissey; p'onse beht; mworse you.\n",
      "  WCALA'. Whees the\n",
      "    You aple,  sword ceeal\n",
      "    Bulk whoth.  [SEILRIO TIADHDENCANLECOS No san that the antuly is louw-    sustney whens at I \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5834/5834 [04:59<00:00, 19.51it/s]\n",
      "100%|██████████| 649/649 [00:09<00:00, 67.38it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss: 0.20791 0.23083\n",
      "      To fly,\n",
      "    Muctertisher you frotitherry dats\n",
      "     mo?\n",
      "  BRLLy, bening thath. TRirking ofsey thour.\n",
      "    creen and whis in\n",
      "    Cown Leath sorret.\n",
      "  BOCIVOCNUD BRIANA. At cathe thes;\n",
      "              he to list wilf seave hasten, harpow nit my yous\n",
      "    wortore. I limlas,\n",
      "    toth all wis tE ont stee tou best;\n",
      "    Tho hit it with'n we? Exook thiy ou swak't deaed ass a dremwast liithee with firse of have pod;\n",
      "    The be dwight, I dey in theof wald you. Hith othise; fall nofre Haf'd ous so nagres youl leceven, mus fare?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "def iterate_batches(data, batch_size, device):\n",
    "    x, y, max_len = [], [], 0\n",
    "    for k in tqdm.tqdm(range(len(data))):\n",
    "        item = data[k]\n",
    "        x.append([sym2idx[sym] for sym in [\"<start>\"] + list(item)])\n",
    "        y.append([sym2idx[sym] for sym in list(item) + [\"<end>\"]])\n",
    "        max_len = max(max_len, len(x[-1]))\n",
    "        if len(x) == batch_size or k + 1 == len(data):\n",
    "            for i in range(len(x)):\n",
    "                x[i] = x[i] + [sym2idx[\"<empty>\"] for _ in range(max_len - len(x[i]))]\n",
    "                y[i] = y[i] + [sym2idx[\"<empty>\"] for _ in range(max_len - len(y[i]))]\n",
    "            x = torch.tensor(x).to(device)\n",
    "            y = torch.tensor(y).to(device)\n",
    "            yield x, y\n",
    "            x, y, max_len = [], [], 0\n",
    "        \n",
    "\n",
    "def train_epoch(data, model):\n",
    "    model.train()\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    optimizer = torch.optim.AdamW(model.parameters())\n",
    "    total_loss, total_count = 0.0, 1e-38\n",
    "    random.shuffle(data)\n",
    "    for inputs, answers in iterate_batches(data, 64, device):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = []\n",
    "        state = model.zero_state(inputs.shape[0]).to(device)\n",
    "        inputs = inputs.transpose(1, 0)\n",
    "        #print(inputs.shape)\n",
    "        #print(answers.shape)\n",
    "        for i in range(inputs.shape[0]):\n",
    "            y, state = model(inputs[i], state)\n",
    "            #print(y.shape)\n",
    "            outputs.append(y)\n",
    "        outputs = torch.stack(outputs).transpose(1, 0).transpose(1, 2)\n",
    "        #print(outputs.shape)\n",
    "        loss = loss_function(outputs, answers)\n",
    "        total_loss += (loss.item() * inputs.shape[0])\n",
    "        total_count += inputs.shape[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss / total_count\n",
    "\n",
    "def test_epoch(data, model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        loss_function = torch.nn.CrossEntropyLoss()\n",
    "        total_loss, total_count = 0.0, 1e-38\n",
    "        for inputs, answers in iterate_batches(data, 64, device):\n",
    "            outputs = []\n",
    "            state = model.zero_state(inputs.shape[0]).to(device)\n",
    "            inputs = inputs.transpose(1, 0)\n",
    "            for i in range(inputs.shape[0]):\n",
    "                y, state = model(inputs[i], state)\n",
    "                outputs.append(y)\n",
    "            outputs = torch.stack(outputs).transpose(1, 0).transpose(1, 2)\n",
    "            loss = loss_function(outputs, answers)\n",
    "            total_loss += (loss.item() * inputs.shape[0])\n",
    "            total_count += inputs.shape[0]\n",
    "        return total_loss / total_count\n",
    "\n",
    "for i in range(10):\n",
    "    train_loss = train_epoch(train, model)\n",
    "    test_loss = test_epoch(test, model)\n",
    "    print(\"Epoch {} loss: {:.5f} {:.5f}\".format(i, train_loss, test_loss))\n",
    "    print(generate(model, 1000))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
